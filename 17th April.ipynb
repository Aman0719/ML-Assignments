{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "2a8e8b92-8f93-413b-a53a-9b870ca0bfb9",
   "metadata": {},
   "source": [
    "#### Q1. What is Gradient Boosting Regression?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1d61d36e-130a-4827-a264-670852c592f7",
   "metadata": {},
   "outputs": [],
   "source": [
    "Ans-\n",
    "\n",
    "Gradient Boosting Regression (GBR) is a machine learning algorithm used for regression tasks. \n",
    "It is a type of boosting algorithm that combines multiple weak predictive models to create a strong predictive model.\n",
    "\n",
    "The idea behind GBR is to iteratively add weak predictive models to the ensemble in a way that corrects the errors made by the previous models.\n",
    "At each iteration, a new model is trained on the residuals (i.e., the difference between the predicted values and the actual values) of the previous models.\n",
    "\n",
    "The name \"gradient boosting\" comes from the fact that the algorithm minimizes the loss function by following the negative gradient of the loss with respect to the predicted values.\n",
    "\n",
    "GBR is a powerful algorithm that can handle complex non-linear relationships between the features and the target variable. \n",
    "It is often used in problems where the number of features is large and the data is noisy.\n",
    "\n",
    "GBR has many hyperparameters that can be tuned to optimize the performance of the model. \n",
    "Some of the important hyperparameters include the number of iterations, the learning rate, the depth of the tree models, and the regularization parameters."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "aa2ef6b1-c9a6-44f8-beea-9ec4dc0b1406",
   "metadata": {},
   "source": [
    "#### Q2. Implement a simple gradient boosting algorithm from scratch using Python and NumPy. Use a simple regression problem as an example and train the model on a small dataset. Evaluate the model's performance using metrics such as mean squared error and R-squared."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "045f087e-5e8b-4776-8403-6196daa012d4",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Mean Squared Error: 0.040\n",
      "R-squared: 0.919\n"
     ]
    }
   ],
   "source": [
    "#Ans-\n",
    "import numpy as np\n",
    "\n",
    "# Generate sample dataset\n",
    "np.random.seed(0)\n",
    "X = np.random.rand(100, 1)\n",
    "y = np.sin(2*np.pi*X).ravel() + np.random.randn(100) * 0.1\n",
    "\n",
    "def gradient_boosting(X, y, n_estimators, learning_rate):\n",
    "    # Initialize the predictions with the mean of the target variable\n",
    "    y_pred = np.full(len(y), np.mean(y))\n",
    "    for i in range(n_estimators):\n",
    "        # Calculate the residual between the true and predicted values\n",
    "        residuals = y - y_pred\n",
    "        \n",
    "        # Fit a simple decision tree to the residuals\n",
    "        tree = DecisionTreeRegressor(max_depth=1)\n",
    "        tree.fit(X, residuals)\n",
    "        \n",
    "        # Update the predictions using the output of the decision tree\n",
    "        y_pred += learning_rate * tree.predict(X)\n",
    "    return y_pred\n",
    "\n",
    "from sklearn.tree import DecisionTreeRegressor\n",
    "from sklearn.metrics import mean_squared_error, r2_score\n",
    "\n",
    "# Train the model\n",
    "y_pred = gradient_boosting(X, y, n_estimators=50, learning_rate=0.1)\n",
    "\n",
    "# Evaluate the model\n",
    "mse = mean_squared_error(y, y_pred)\n",
    "r2 = r2_score(y, y_pred)\n",
    "print(f\"Mean Squared Error: {mse:.3f}\")\n",
    "print(f\"R-squared: {r2:.3f}\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4603160a-64e6-4c09-af66-bc8f900bb9d5",
   "metadata": {},
   "source": [
    "#### Q3. Experiment with different hyperparameters such as learning rate, number of trees, and tree depth to optimise the performance of the model. Use grid search or random search to find the best hyperparameters"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "bdb47e37-3431-48c0-9720-8b040dc3ef78",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Best parameters:  {'n_estimators': 150, 'max_depth': 1, 'learning_rate': 0.1}\n",
      "Best score:  0.9600861804922933\n"
     ]
    }
   ],
   "source": [
    "#Ans\n",
    "\n",
    "from sklearn.model_selection import RandomizedSearchCV\n",
    "from sklearn.ensemble import GradientBoostingRegressor\n",
    "\n",
    "# Define the parameter grid\n",
    "param_grid = {\n",
    "    'n_estimators': [50, 100, 150, 200],\n",
    "    'learning_rate': [0.01, 0.1, 0.5],\n",
    "    'max_depth': [1, 2, 3, 4, 5]\n",
    "}\n",
    "\n",
    "# Create the model and perform random search\n",
    "model = GradientBoostingRegressor()\n",
    "random_search = RandomizedSearchCV(estimator=model, param_distributions=param_grid, n_iter=20, cv=5, random_state=0)\n",
    "random_search.fit(X, y)\n",
    "\n",
    "# Print the best parameters and score\n",
    "print(\"Best parameters: \", random_search.best_params_)\n",
    "print(\"Best score: \", random_search.best_score_)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ca25a888-0288-48e7-871c-da188c737a0e",
   "metadata": {},
   "source": [
    "#### Q4. What is a weak learner in Gradient Boosting?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fd89b7a7-f0ff-4438-bda9-a8b3b0c8440c",
   "metadata": {},
   "outputs": [],
   "source": [
    "Ans-\n",
    "\n",
    "Gradient Boosting is an ensemble method that combines multiple weak learners to create a strong predictive model.\n",
    "A weak learner is a simple model that performs slightly better than random guessing.\n",
    "In the context of Gradient Boosting, a weak learner is a decision tree with a limited number of splits, also known as a \"shallow tree\".\n",
    "\n",
    "In Gradient Boosting, the idea is to sequentially add weak learners to the ensemble, with each new learner attempting to correct the errors made by the previous learners.\n",
    "The algorithm assigns weights to the training examples based on their relative importance, with the goal of giving more weight to the examples that are difficult to classify correctly.\n",
    "\n",
    "The process of adding new weak learners to the ensemble is repeated until the desired level of performance is achieved, or until the algorithm reaches a predefined number of iterations.\n",
    "The final prediction is a weighted average of the predictions of all the weak learners in the ensemble.\n",
    "\n",
    "In summary, a weak learner in Gradient Boosting is a decision tree with a limited number of splits, and it is used in combination with other weak learners to create a strong predictive model."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "80b88c1d-d405-4ea4-964d-5f68d16827bd",
   "metadata": {},
   "source": [
    "#### Q5. What is the intuition behind the Gradient Boosting algorithm?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8e209491-7f8d-44f3-b058-96af5e4cd260",
   "metadata": {},
   "outputs": [],
   "source": [
    "Ans-\n",
    "\n",
    "The Gradient Boosting algorithm is an ensemble method that combines multiple weak learners to create a strong predictive model. \n",
    "The intuition behind Gradient Boosting is to iteratively add weak learners to the ensemble, with each new learner attempting to correct the errors made by the previous learners.\n",
    "\n",
    "The algorithm works by minimizing a cost function, which measures the difference between the predicted and actual values of the target variable.\n",
    "In each iteration, the algorithm fits a new weak learner to the residuals, which are the differences between the predicted and actual values of the target variable.\n",
    "The idea is that the new learner will learn to predict the residual errors made by the previous learners, thus reducing the overall error of the ensemble.\n",
    "\n",
    "The algorithm assigns weights to the training examples based on their relative importance, with the goal of giving more weight to the examples that are difficult to classify correctly.\n",
    "This allows the algorithm to focus on the examples that are most important for improving the performance of the model.\n",
    "\n",
    "The final prediction is a weighted sum of the predictions of all the weak learners in the ensemble. \n",
    "The weights are determined by the performance of each learner on the training examples, with the best-performing learners given more weight in the final prediction.\n",
    "\n",
    "In summary, the intuition behind Gradient Boosting is to iteratively add weak learners to the ensemble, with each new learner attempting to correct the errors made by the previous learners.\n",
    "The algorithm assigns weights to the training examples based on their relative importance and minimizes a cost function by fitting new learners to the residual errors. \n",
    "The final prediction is a weighted sum of the predictions of all the weak learners in the ensemble."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "558e6904-a73d-42b9-841c-cdd0d0e3f46f",
   "metadata": {},
   "source": [
    "#### Q6. How does Gradient Boosting algorithm build an ensemble of weak learners?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b38206ae-eb92-428f-a90e-ebcfff37eb90",
   "metadata": {},
   "outputs": [],
   "source": [
    "Ans-\n",
    "\n",
    "The Gradient Boosting algorithm builds an ensemble of weak learners by iteratively adding new learners to the ensemble, with each new learner attempting to correct the errors made by the previous learners.\n",
    "The steps involved in building an ensemble of weak learners using Gradient Boosting are as follows:\n",
    "\n",
    "1.Initialize the ensemble with a simple model:\n",
    "The algorithm starts by initializing the ensemble with a simple model, such as a decision tree with a single split.\n",
    "This model is often called the \"base learner\" or the \"initial model.\"\n",
    "\n",
    "2.Calculate the residuals:\n",
    "The algorithm calculates the difference between the predicted and actual values of the target variable for each training example. \n",
    "These differences are called \"residuals.\"\n",
    "\n",
    "3.Fit a new weak learner to the residuals: \n",
    "The algorithm fits a new weak learner to the residuals, with the goal of predicting the errors made by the previous learner.\n",
    "The new learner is usually a decision tree with a limited number of splits, also known as a \"shallow tree.\"\n",
    "\n",
    "4.Update the ensemble:\n",
    "The algorithm adds the new learner to the ensemble and combines it with the previous learners.\n",
    "This is done by taking a weighted sum of the predictions of all the learners in the ensemble, where the weights are determined by the performance of each learner on the training examples.\n",
    "\n",
    "5.Repeat steps 2-4: \n",
    "The algorithm repeats steps 2-4 until a predefined stopping criterion is met, such as a maximum number of iterations or a minimum level of performance.\n",
    "\n",
    "6.Make predictions:\n",
    "The final prediction is a weighted sum of the predictions of all the learners in the ensemble. \n",
    "The weights are determined by the performance of each learner on the training examples, with the best-performing learners given more weight in the final prediction.\n",
    "\n",
    "In summary, the Gradient Boosting algorithm builds an ensemble of weak learners by iteratively adding new learners to the ensemble, with each new learner attempting to correct the errors made by the previous learners.\n",
    "The algorithm calculates the residuals, fits a new weak learner to the residuals, updates the ensemble, and repeats the process until a stopping criterion is met. \n",
    "The final prediction is a weighted sum of the predictions of all the learners in the ensemble."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fc447a36-b05d-42f0-9e60-6b2aa7e270f1",
   "metadata": {},
   "source": [
    "#### Q7. What are the steps involved in constructing the mathematical intuition of Gradient Boosting algorithm?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1ac811a8-1a0f-49b6-ad2b-1f3692cee6cf",
   "metadata": {},
   "outputs": [],
   "source": [
    "Ans-\n",
    "\n",
    "The mathematical intuition behind Gradient Boosting algorithm involves the following steps:\n",
    "\n",
    "1.Define a loss function: \n",
    "The first step is to define a loss function that measures the difference between the predicted and actual values of the target variable. \n",
    "This loss function is typically a differentiable function that can be optimized using gradient descent.\n",
    "\n",
    "2.Initialize the model: \n",
    "The algorithm starts by initializing the model with a simple function, such as a constant value or the mean of the target variable.\n",
    "\n",
    "3.Calculate the negative gradient of the loss function: \n",
    "The negative gradient of the loss function is calculated with respect to the predictions of the current model.\n",
    "This negative gradient represents the direction in which the loss function decreases the most.\n",
    "\n",
    "4.Fit a weak learner to the negative gradient: \n",
    "The algorithm fits a weak learner to the negative gradient, with the goal of predicting the direction in which the loss function decreases the most.\n",
    "The weak learner is usually a decision tree with a limited number of splits, also known as a \"shallow tree.\"\n",
    "\n",
    "5.Update the model: \n",
    "The algorithm updates the model by adding the predictions of the weak learner, multiplied by a learning rate, to the current model.\n",
    "The learning rate determines the contribution of the weak learner to the final model and is usually set to a small value, such as 0.1.\n",
    "\n",
    "6.Repeat steps 3-5:\n",
    "The algorithm repeats steps 3-5 until a predefined stopping criterion is met, such as a maximum number of iterations or a minimum level of performance.\n",
    "\n",
    "7.Make predictions:\n",
    "The final prediction is the sum of the predictions of all the weak learners in the ensemble, each multiplied by its learning rate.\n",
    "\n",
    "In summary, the mathematical intuition behind Gradient Boosting algorithm involves defining a loss function, initializing the model, calculating the negative gradient of the loss function, \n",
    "fitting a weak learner to the negative gradient, updating the model, and repeating the process until a stopping criterion is met. \n",
    "The final prediction is the sum of the predictions of all the weak learners in the ensemble, each multiplied by its learning rate."
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
